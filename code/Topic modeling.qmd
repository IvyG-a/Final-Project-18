---
title: "Topic Modeling"
format: pdf
editor: visual
---

```{r setup, include=FALSE}
library(quanteda)
library(tidyverse)
library(lubridate)
library(topicmodels)
library(dplyr)
library(stringr)
#library(RTextTools)
```

```{r}
data <- read.csv("cleaned_comments.csv")
str(data)
colnames(data)
head(data)
```

```{r}
# data cleaning for topic modeling
data_pre <- data %>%

  dplyr::select(author, date, score, upvotes, downvotes, comment) %>%
  
  mutate(
    day = lubridate::ymd(date),
    month = as.numeric(lubridate::month(date, label = TRUE, abbr = TRUE)),
    year = lubridate::year(date)
  ) %>%
  
  mutate(
    comment = stringr::str_trim(
      stringr::str_to_lower(
        stringr::str_replace_all(comment, "[^a-zA-Z0-9\\s]", "")
      )
    )
  ) %>%
  

  mutate(doc_id = paste0("doc_", seq_len(n()))) %>%
  

  filter(!is.na(comment) & nchar(comment) > 10)
```

```{r}
glimpse(data_pre)
head(data_pre)
```

Calculate number of characters in each text and sort dataset.

```{r}
data_pre %>%
  mutate(n_char = nchar(comment)) %>%
  group_by(doc_id) %>%
  summarise(median(n_char))
```

```{r}
corpus_comments <- corpus(data_pre, 
                          docid_field = "doc_id", 
                          text_field = "comment")    


tokens_comments <- tokens(corpus_comments, 
                          remove_punct = TRUE,  
                          remove_numbers = TRUE,
                          remove_symbols = TRUE) %>% 
                   tokens_tolower() 

lemmaData <- read.csv("baseform_en.tsv",
                      sep = ",", 
                      header = FALSE, 
                      encoding = "UTF-8", 
                      stringsAsFactors = FALSE)

```


```{r}
tokens_comments <- tokens_replace(tokens_comments, 
                                   lemmaData$V1,  
                                   lemmaData$V2,
                                   valuetype = "fixed")


tokens_comments <- tokens_comments %>%
  tokens_remove(pattern = english, valuetype = "fixed") %>%
  tokens_ngrams(n = 1)

library(stopwords)

tokens_comments <- tokens_comments %>%
  tokens_remove(pattern = stopwords("en"), valuetype = "fixed") %>%
  tokens_ngrams(n = 1)


print(head(tokens_comments))

dtm_comments <- dfm(tokens_comments)

print(dtm_comments)
```

```{r}
# write.csv(convert(dtm_comments, to = "matrix"), "dtm_comments.csv", row.names = TRUE)
```

```{r}
#  Create dtm
DTM <- dfm(dtm_comments)

DTM <- dfm_trim(DTM, min_docfreq = 10, max_docfreq = 0.7 * nrow(DTM))

DTM  <- dfm_select(DTM, 
                   pattern = "[a-z]", 
                   valuetype = "regex", 
                   selection = 'keep')
colnames(DTM) <- stringi::stri_replace_all_regex(colnames(DTM), 
                                                 "[^_a-z]","")

DTM <- dfm_compress(DTM, "features")


sel_idx <- rowSums(DTM) > 0
DTM <- DTM[sel_idx, ]
textdata <- data[sel_idx, ]
```

```{r}
K <- 12
set.seed(9161)
topicModel <- LDA(DTM, 
                  K, 
                  method = "Gibbs", 
                  control = list(iter = 500, verbose = 25))

```

```{r}
tmResult <- posterior(topicModel)

beta <- tmResult$terms
glimpse(beta)            

theta <- tmResult$topics
glimpse(theta)               

terms(topicModel, 10)

top5termsPerTopic <- terms(topicModel, 
                           5)

topicNames <- apply(top5termsPerTopic, 
                    2, 
                    paste, 
                    collapse=" ")
```

```{r}
topicProportions <- colSums(theta) / nrow(DTM)
names(topicProportions) <- topicNames     
sort(topicProportions, decreasing = TRUE) 
```

```{r}
attr(topicModel, "alpha") 

topicModel2 <- LDA(DTM, 
                   K, 
                   method="Gibbs", 
                   control=list(iter = 1000, 
                                verbose = 25, 
                                alpha = 0.2))
tmResult <- posterior(topicModel2)
theta <- tmResult$topics
beta <- tmResult$terms


topicProportions <- colSums(theta) / nrow(DTM)  
names(topicProportions) <- topicNames     
sort(topicProportions, decreasing = TRUE) 

topicNames <- apply(terms(topicModel2, 10), 2, paste, collapse = " ")
```

```{r}
terms(topicModel2, 10)
```

```{r}
str(DTM)
```
# visualization
```{r fig.height=8,fig.width=16}
library(dplyr)
library(tidyr)
library(ggplot2)


theta_df <- as.data.frame(theta)
colnames(theta_df) <- paste0("Topic_", seq_len(ncol(theta)))  
theta_df$date <- as.Date(DTM@docvars$date)  

theta_df <- theta_df %>%
  filter(date >= as.Date("2018-01-01") & date <= as.Date("2024-11-23")) %>%  
  mutate(month = format(date, "%Y-%m")) %>% 
  group_by(month) %>%
  summarise(across(starts_with("Topic_"), mean, na.rm = TRUE))  

theta_long <- theta_df %>%
  pivot_longer(cols = starts_with("Topic_"), names_to = "Topic", values_to = "Frequency")

ggplot(theta_long, aes(x = month, y = Frequency, color = Topic, group = Topic)) +
  geom_line() +
  scale_x_discrete(breaks = theta_df$month[seq(1, nrow(theta_df), by = 6)]) +
  geom_vline(xintercept = c("2020-01", "2023-05"), 
             linetype = "dashed", color = "red", size = 0.8) +
  labs(title = "Topic Frequency Over Time (Monthly)",
       x = "Month",
       y = "Frequency",
       color = "Topic") +
  theme_minimal() +

  theme(
    plot.title = element_text(hjust = 0.5, size = 20, face = "bold"),  
    axis.title.x = element_text(size = 18),                            
    axis.title.y = element_text(size = 18),                           
    axis.text.x = element_text(angle = 45, hjust = 1, size = 14),      
    axis.text.y = element_text(size = 16),                           
    legend.title = element_text(size = 18),                            
    legend.text = element_text(size = 16)                             
  )

```
